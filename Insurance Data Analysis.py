# -*- coding: utf-8 -*-
"""Insurance Data Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15An-Gh5krdIqi_fIoK0-zOvuTg5uxX5E
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

data = pd.read_csv("./InsNova_data_2023_train.csv")

data.head()

"""- response variable of interest: claim cost (claimcst0)
  - in validation set, clm (claim indicator) and numclaims (number of claims) also omitted

- 45,239 policies, about 6.8 percent had at least one claim

- explanatory variables:
  - numeric:
    - veh_value: market value of vehicle in 10,000s, right-skewed dist
    - exposure: basic unit of risk underlying insurance premium (between 0 and 1)-- not sure I understand what this variable is
    - max_power: max horsepower of vehicle (what are the units?)
    - driving_history_score: driving score based on past driving history (higher = better)
    - credit_score
  - categorical:
    - binary:
      - gender (M/F)
      - marital_status (M/S)
      - e_bill: indicator for paperless billing (0 = no, 1 = yes)
      - time_of_week_driven: most frequent driving day of week (weekday/weekend)
      - trm_len: term length for policy (6m/12m)
      - high_education_ind: indicator of higher education
    - multiple categories:
      - ordinal (factors):
        - veh_age: 4 categories, 1 = youngest, 4 = oldest
        - agecat: driver's age category, 6 categories, 1 = youngest, 6 = oldest
      - nominal:
        - veh_body: type of vehicle, 5 categories: SEDAN, HBACK, STNWG, UTE, TRUCK
        - area: driving area of residence, 5 categories: A, B, D, C, E (do not know what these mean)
        - engine_type: engine type of vehicle, 4 categories: petrol, dissel, hybrid, electric
        - veh_color: color of vehicle, 5 categories: white, gray, black, blue, silver (does not seem to include all possible colors?)
        - time_driven: most frequent driving time of day, 4 categories: 12pm-6pm, 6pm-12am, 12am-6am, 6am-12pm
"""

data_1s = data.copy()
data_1s = data_1s[data_1s['claimcst0'] > 0]

"""Exploratory Data Analysis (EDA)"""

print(data.info())

# Check for missing values
missing_values = data.isnull().sum()
print(missing_values[missing_values > 0])

"""From these results, we can see that there are no missing values in the dataset. We are continuing our analysis to check for any unusual observations.

Boxplots for numerical variables
"""

numerical_columns = ['veh_value', 'exposure', 'max_power', 'driving_history_score', 'credit_score']
plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_columns):
    plt.subplot(len(numerical_columns), 1, i+1)
    sns.boxplot(x=data[col])
    plt.title(f'Boxplot of {col}')
    plt.tight_layout()
plt.show()

"""From these boxplots, we can observe that 'veh_value', 'max_power', 'driving_history_score', and 'credit_score' have outliers. In the context of our data, these are possible values, not obvious errors. Therefore, these data points are very important, and we are keeping them in the dataset. We have to keep in mind that in the model selection, we need to choose a model that is robust to outliers and can handle them effectively.

Histograms for numerical variables
"""

columns = ['veh_value', 'exposure', 'max_power', 'driving_history_score', 'credit_score']
plt.figure(figsize=(15, 10))
n_rows = int(len(columns) / 2) + (len(columns) % 2 > 0)

for i, column in enumerate(columns):
    ax = plt.subplot(n_rows, 2, i + 1)
    sns.histplot(data[column], kde=True, ax=ax)  # Kernel density function (smoothed version of the histogram)
    ax.set_title(f'Distribution of {column}')

plt.tight_layout()
plt.show()

"""From these histograms, we can see that vehicle value and credit score have smooth, right-skewed distributions. Exposure and maximum power have less smooth, right-skewed distributions. Finally, the driving history score has a left-skewed distribution.

Binary categorical variables
"""

columns = ['gender', 'marital_status', 'e_bill', 'time_of_week_driven', 'trm_len', 'high_education_ind']
plt.figure(figsize=(15, 10))
n_rows = int(len(columns) / 2) + (len(columns) % 2 > 0)

for i, column in enumerate(columns):
    ax = plt.subplot(n_rows, 2, i+1)
    sns.countplot(x=column, data=data, order=data[column].value_counts().index)
    plt.title(f'Frequency of {column}')

    total = len(data[column])
    for p in ax.patches:
        percentage = f'{100 * p.get_height() / total:.1f}%'
        x = p.get_x() + p.get_width() / 2
        y = p.get_y() + p.get_height()
        ax.annotate(percentage, (x, y), ha='center', va='bottom')

plt.tight_layout()
plt.show()

"""From the bar charts, it is apparent that the sample contains a greater proportion of females than males, with a notable difference of 13.6%. Additionally, the majority of individuals in the dataset are married, exhibiting a difference of 9.6%. When it comes to billing methods, most are enrolled in e-billing, with a substantial margin of 27.4%. In terms of driving habits, weekday driving predominates over weekend driving by a significant 60%. Furthermore, a 6-month term length is the more common policy duration, with a difference of 49.8%. Lastly, a large majority of 76.6% of the dataset's individuals do not possess higher education credentials.

Nominal and Ordinal categorical variables
"""

columns = ['veh_age', 'agecat', 'veh_body', 'area', 'engine_type', 'veh_color', 'time_driven']
plt.figure(figsize=(15, 10))

for i, column in enumerate(columns):
    ax = plt.subplot(3, 3, i+1)
    sns.countplot(x=column, data=data, order=data[column].value_counts().index)
    plt.title(f'Count of {column}')
    plt.xticks(rotation=90)  #Rotate x labels for better readability

    total = float(len(data[column]))
    for p in ax.patches:
        percentage = f'{100 * p.get_height() / total:.1f}%'
        x = p.get_x() + p.get_width() / 2
        y = p.get_y() + p.get_height()
        ax.annotate(percentage, (x, y), ha='center', va='bottom')

plt.tight_layout()
plt.show()

"""Relationship Between Variables

Scatter plots for numerical variables vs claim_cost
"""

columns = ['veh_value', 'exposure', 'max_power', 'driving_history_score', 'credit_score']
plt.figure(figsize=(15, 10))
n_rows = int(len(columns) / 2) + (len(columns) % 2 > 0)

for i, column in enumerate(columns):
    ax = plt.subplot(n_rows, 2, i + 1)
    sns.scatterplot(x=data[column], y=data['claimcst0'], ax=ax)
    ax.set_title(f'{column} vs Claim Cost')

plt.tight_layout()
plt.show()

columns = ['veh_value', 'exposure', 'max_power', 'driving_history_score', 'credit_score']
plt.figure(figsize=(15, 10))
n_rows = int(len(columns) / 2) + (len(columns) % 2 > 0)

for i, column in enumerate(columns):
    ax = plt.subplot(n_rows, 2, i + 1)
    sns.scatterplot(x=data_1s[column], y=data_1s['claimcst0'], ax=ax)
    ax.set_title(f'{column} vs Claim Cost')

plt.tight_layout()
plt.show()

"""Binary Response"""

columns = ['veh_value', 'exposure', 'max_power', 'driving_history_score', 'credit_score']
plt.figure(figsize=(15, 10))
n_rows = int(len(columns) / 2) + (len(columns) % 2 > 0)

for i, column in enumerate(columns):
    ax = plt.subplot(n_rows, 2, i + 1)
    sns.boxplot(x=data['clm'], y=data[column], ax=ax)
    ax.set_title(f'{column} vs Has Claim')

plt.tight_layout()
plt.show()

"""The scatter plots do not reveal any clear linear relationships between the numerical variables and the claim cost. It appears that the data points are scattered broadly without forming any distinct patterns that would suggest a direct proportional or inversely proportional relationship. This lack of observable linearity indicates that more complex models or non-linear analysis may be necessary to accurately predict claim costs from these variables. It's also possible that interactions between variables or other, non-considered factors could play a significant role in determining the claim cost.

Binary Variables vs Claim cost
"""

binary_columns = ['gender', 'marital_status', 'e_bill', 'time_of_week_driven', 'trm_len', 'high_education_ind']
plt.figure(figsize=(15, 10))
n_rows = int(len(binary_columns) / 2) + (len(binary_columns) % 2 > 0)

for i, column in enumerate(binary_columns):
    ax = plt.subplot(n_rows, 2, i + 1)
    sns.boxplot(x=data[column], y=data['claimcst0'], ax=ax)
    ax.set_title(f'Claim Cost by {column}')

plt.tight_layout()
plt.show()

"""Binary Response"""

binary_columns = ['gender', 'marital_status', 'e_bill', 'time_of_week_driven', 'trm_len', 'high_education_ind']
plt.figure(figsize=(15, 10))
n_rows = int(len(binary_columns) / 2) + (len(binary_columns) % 2 > 0)

for i, column in enumerate(binary_columns):
    prop_data = (data.groupby([column])['clm']
                     .value_counts(normalize = True)
                     .rename('percentage')
                     .mul(100)
                     .reset_index()
                     )
    ax = plt.subplot(n_rows, 2, i + 1)
    sns.barplot(x = prop_data['clm'], y = prop_data['percentage'], hue=prop_data[column], ax = ax)
    for p in ax.patches:
      ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+.12, p.get_height()+.5))
    ax.set_title(f'Percentage with a Claim by {column}')

plt.tight_layout()
plt.show()

prop_data

binary_columns = ['gender', 'marital_status', 'e_bill', 'time_of_week_driven', 'trm_len', 'high_education_ind']
plt.figure(figsize=(15, 10))
n_rows = int(len(binary_columns) / 2) + (len(binary_columns) % 2 > 0)

for i, column in enumerate(binary_columns):
    ax = plt.subplot(n_rows, 2, i + 1)
    sns.boxplot(x=data_1s[column], y=data_1s['claimcst0'], ax=ax)
    ax.set_title(f'Claim Cost by {column}')

plt.tight_layout()
plt.show()

"""Nominal and Ordinal Variables vs Claim cost"""

nominal_and_ordinal_columns = [
    'veh_age',  # ordinal
    'agecat',   # ordinal
    'veh_body', # nominal
    'area',     # nominal
    'engine_type', # nominal
    'veh_color',   # nominal
    'time_driven'  # nominal
]

plt.figure(figsize=(15, 15))
n_rows = int(len(nominal_and_ordinal_columns) / 2) + (len(nominal_and_ordinal_columns) % 2 > 0)

for i, column in enumerate(nominal_and_ordinal_columns):
    ax = plt.subplot(n_rows, 2, i + 1)
    sns.boxplot(x=data[column], y=data['claimcst0'], ax=ax)
    ax.set_title(f'Claim Cost by {column}')
    plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right')

plt.tight_layout()
plt.show()

"""Binary Response"""

nominal_and_ordinal_columns = [
    'veh_age',  # ordinal
    'agecat',   # ordinal
    'veh_body', # nominal
    'area',     # nominal
    'engine_type', # nominal
    'veh_color',   # nominal
    'time_driven'  # nominal
]

plt.figure(figsize=(15, 15))
n_rows = int(len(nominal_and_ordinal_columns) / 2) + (len(nominal_and_ordinal_columns) % 2 > 0)

for i, column in enumerate(nominal_and_ordinal_columns):
  prop_data = (data.groupby([column])['clm']
                     .value_counts(normalize = True)
                     .rename('percentage')
                     .mul(100)
                     .reset_index()
                     )

  ax = plt.subplot(n_rows, 2, i + 1)
  sns.barplot(x = prop_data['clm'], y = prop_data['percentage'], hue=prop_data[column], ax = ax)

  ax.set_title(f'Percentage with a Claim by {column}')
  plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right')

plt.tight_layout()
plt.show()

nominal_and_ordinal_columns = [
    'veh_age',  # ordinal
    'agecat',   # ordinal
    'veh_body', # nominal
    'area',     # nominal
    'engine_type', # nominal
    'veh_color',   # nominal
    'time_driven'  # nominal
]

plt.figure(figsize=(15, 15))
n_rows = int(len(nominal_and_ordinal_columns) / 2) + (len(nominal_and_ordinal_columns) % 2 > 0)

for i, column in enumerate(nominal_and_ordinal_columns):
    ax = plt.subplot(n_rows, 2, i + 1)
    sns.boxplot(x=data_1s[column], y=data_1s['claimcst0'], ax=ax)
    ax.set_title(f'Claim Cost by {column}')
    plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right')

plt.tight_layout()
plt.show()

"""Correlations and ANOVA"""

pip install pandas scipy statsmodels

import pandas as pd
import scipy.stats as stats
import statsmodels.api as sm
from statsmodels.formula.api import ols

df = pd.read_csv('./InsNova_data_2023_train.csv')

# 1. Correlation Coefficient for Numeric Variables
numeric_vars = ['veh_value', 'exposure', 'max_power', 'driving_history_score', 'credit_score']
for var in numeric_vars:
    correlation, p_value = stats.pearsonr(df[var], df['claimcst0'])
    print(f"Correlation between {var} and claimcst0: {correlation}, P-value: {p_value}")

# 2. ANOVA for Categorical Variables
categorical_vars = ['gender', 'marital_status', 'e_bill', 'time_of_week_driven', 'trm_len',
                    'high_education_ind', 'veh_age', 'agecat', 'veh_body', 'area',
                    'engine_type', 'veh_color', 'time_driven']

for var in categorical_vars:
    model = ols(f'claimcst0 ~ C({var})', data=df).fit()
    anova_table = sm.stats.anova_lm(model, typ=2)
    print(f"ANOVA for {var}:")
    print(anova_table)

"""The distribution of the target variable"""

import matplotlib.pyplot as plt
import seaborn as sns

# Set the style for seaborn plots
sns.set_style("whitegrid")

# Plot the distribution of the target variable 'claimcst0'
plt.figure(figsize=(10, 6))
sns.histplot(data['claimcst0'], bins=50, kde=True)
plt.title('Distribution of Claim Amounts')
plt.xlabel('Claim Amount')
plt.ylabel('Frequency')
plt.show()

"""The distribution of claim amounts shows a heavy right-skew, indicating that a majority of the claims are concentrated around smaller values, with a few larger claims. This is typical for insurance data, where many policies have no claims or small claims, and a few have large claims."""

# Plot the distribution of the target variable 'claimcst0'
plt.figure(figsize=(10, 6))
sns.histplot(data_1s['claimcst0'], bins=50, kde=True)
plt.title('Distribution of Claim Amounts for Cost > 0')
plt.xlabel('Claim Amount')
plt.ylabel('Frequency')
plt.show()

"""# Target
- predicting the claim cost for each policy and to convince your business partner that your predictions will work well.
- Target: Build a model to predict the claim cost and submit the predicted cost for the Validation data

# Benchmark Model
- LightGBM model
- (Alternative: XGboost(Extreme Gradient Boosting), GBDT(Gradient Boosting Decision Tree))

# Evaluation:
- Gini index

# Evaluation
"""

# from https://www.kaggle.com/code/batzner/gini-coefficient-an-intuitive-explanation
def gini(actual, pred):
    assert (len(actual) == len(pred))
    all = np.asarray(np.c_[actual, pred, np.arange(len(actual))], dtype=float)
    all = all[np.lexsort((all[:, 2], -1 * all[:, 1]))]
    totalLosses = all[:, 0].sum()
    giniSum = all[:, 0].cumsum().sum() / totalLosses

    giniSum -= (len(actual) + 1) / 2.
    return giniSum / len(actual)


def gini_normalized(actual, pred):
    return gini(actual, pred) / gini(actual, actual)

gini_predictions = gini(actual, predictions)
gini_max = gini(actual, actual)
ngini= gini_normalized(actual, predictions)
print('Gini: %.3f, Max. Gini: %.3f, Normalized Gini: %.3f' % (gini_predictions, gini_max, ngini))

"""LightGBM Tuning"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import LabelEncoder
import lightgbm as lgb

def gini(actual, pred):
    assert (len(actual) == len(pred))
    all = np.asarray(np.c_[actual, pred, np.arange(len(actual))], dtype=float)
    all = all[np.lexsort((all[:, 2], -1 * all[:, 1]))]
    totalLosses = all[:, 0].sum()
    giniSum = all[:, 0].cumsum().sum() / totalLosses

    giniSum -= (len(actual) + 1) / 2.
    return giniSum / len(actual)


def gini_normalized(actual, pred):
    return gini(actual, pred) / gini(actual, actual)

# Load and prepare the data
train_df = pd.read_csv('/InsNova_data_2023_train.csv')
y_train = train_df['claimcst0']
X_train = train_df.drop(['claimcst0', 'clm', 'numclaims', 'id'], axis=1)

# Split the training data to have a validation set
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# Encoding categorical variables for both training and validation sets
categorical_columns = [
    'gender', 'marital_status', 'e_bill', 'time_of_week_driven', 'trm_len', 'high_education_ind',
    'veh_age', 'agecat',
    'veh_body', 'area', 'engine_type', 'veh_color', 'time_driven'
]
label_encoders = {}
for column in categorical_columns:
    le = LabelEncoder()
    X_train[column] = le.fit_transform(X_train[column])
    X_val[column] = le.transform(X_val[column])  # Use the fitted LabelEncoder
    label_encoders[column] = le

# Hyperparameter grid
num_leaves_options = [31, 50, 70]
min_data_in_leaf_options = [20, 30, 50]
learning_rate_options = [0.01, 0.05, 0.1]

best_gini = -np.inf
best_params = {}
best_iteration = None

for num_leaves in num_leaves_options:
    for min_data_in_leaf in min_data_in_leaf_options:
        for learning_rate in learning_rate_options:
            print(f"Training with num_leaves: {num_leaves}, min_data_in_leaf: {min_data_in_leaf}, learning_rate: {learning_rate}")

            params = {
                'boosting_type': 'gbdt',
                'objective': 'regression',
                'metric': 'rmse',
                'num_leaves': num_leaves,
                'min_data_in_leaf': min_data_in_leaf,
                'learning_rate': learning_rate,
                'feature_fraction': 0.9,
                'bagging_fraction': 0.8,
                'bagging_freq': 5,
                'verbose': 0,
                'lambda_l1': 2.0,
                'lambda_l2': 2.0,
            }

            lgb_train = lgb.Dataset(X_train, y_train)
            lgb_val = lgb.Dataset(X_val, y_val, reference=lgb_train)

            model = lgb.train(
                params, lgb_train, num_boost_round=1000,
                valid_sets=[lgb_val],
                callbacks=[
                    lgb.early_stopping(stopping_rounds=50),
                    lgb.log_evaluation(period=100)  # This will print the evaluation log every 100 iterations
                ]
            )

            y_pred = model.predict(X_val, num_iteration=model.best_iteration)
            gini_score = gini(y_val, y_pred)
            print(f"Gini score: {gini_score}")

            if gini_score > best_gini:
                best_gini = gini_score
                best_params = {
                    'num_leaves': num_leaves,
                    'min_data_in_leaf': min_data_in_leaf,
                    'learning_rate': learning_rate
                }
                best_iteration = model.best_iteration
                best_rmse = model.best_score['valid_0']['rmse']

print(f"Best Gini score: {best_gini}")
print(f"Best parameters: {best_params}")
print(f"Best iteration: {best_iteration}")
print(f"Best RMSE: {best_rmse}")

"""Adding the parameter values to the model"""

import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split


# Load training data
train_df = pd.read_csv('/InsNova_data_2023_train.csv')

# Split the training data into new training and validation sets
train_set, validation_set = train_test_split(train_df, test_size=0.2, random_state=42)

# Target variable is 'claim_cost' in our training data
y_train = train_set['claimcst0']
y_validation = validation_set['claimcst0']

# Drop the target and other related columns, using 'id'
features_to_drop = ['claimcst0', 'clm', 'numclaims', 'id']  # 'id' is the identifier column
X_train = train_set.drop(features_to_drop, axis=1)
X_validation = validation_set.drop(features_to_drop, axis=1)

# Encode categorical variables
categorical_columns = [
    'gender', 'marital_status', 'e_bill', 'time_of_week_driven', 'trm_len', 'high_education_ind',
    'veh_age', 'agecat',
    'veh_body', 'area', 'engine_type', 'veh_color', 'time_driven'
]
label_encoders = {}
for column in categorical_columns:
    le = LabelEncoder()
    X_train[column] = le.fit_transform(train_set[column])
    X_validation[column] = le.transform(validation_set[column])
    label_encoders[column] = le  # Store the label encoder

# Define parameters for the LightGBM model
params = {
    'boosting_type': 'gbdt',
    'objective': 'regression',
    'metric': 'rmse',
    'num_leaves': 31,
    'min_data_in_leaf': 20,
    'learning_rate': 0.1,
    'feature_fraction': 0.99,
    'bagging_fraction': 0.5,
    'bagging_freq': 5,
    'verbose': 0,
    'lambda_l1': 2.0,
    'lambda_l2': 2.0,
    'feature_fraction_bynode': 0.92
}

# Create LightGBM datasets
lgb_train = lgb.Dataset(X_train, y_train)
lgb_validation = lgb.Dataset(X_validation, y_validation)  # Use y_validation here

# Train the model with early stopping(To prevent overfitting)
num_round = 1000
bst = lgb.train(
    params, lgb_train, num_round,
    valid_sets=[lgb_validation],
    callbacks=[lgb.early_stopping(stopping_rounds=50)]
)

# Predict on new validation data
ypred = bst.predict(X_validation, num_iteration=bst.best_iteration)

# Calculate the Gini coefficient for the new validation set
gini_predictions = gini(y_validation, ypred)
gini_max = gini(y_validation, y_validation)
ngini= gini_normalized(y_validation, ypred)
print('Gini: %.3f, Max. Gini: %.3f, Normalized Gini: %.3f' % (gini_predictions, gini_max, ngini))