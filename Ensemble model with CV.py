# -*- coding: utf-8 -*-
"""Final_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ds_i95LdkVzBIqaqKJpbuBCVMQxO2TXn
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import roc_auc_score, mean_squared_error
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import LabelEncoder
import xgboost as xgb
from sklearn.metrics import accuracy_score
from xgboost import XGBClassifier
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingClassifier
from xgboost.sklearn import XGBClassifier
from sklearn.metrics import roc_auc_score
from sklearn.neural_network import MLPRegressor
from sklearn import tree
import math
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from xgboost import XGBRegressor
from sklearn.model_selection import KFold
import lightgbm as lgb
from lightgbm import early_stopping
from sklearn.preprocessing import RobustScaler
import statsmodels.api as sm
from statsmodels.genmod.families import Tweedie
from statsmodels.genmod.families.links import log

def gini(actual, pred):
    assert (len(actual) == len(pred))
    all = np.asarray(np.c_[actual, pred, np.arange(len(actual))], dtype=float)
    all = all[np.lexsort((all[:, 2], -1 * all[:, 1]))]
    totalLosses = all[:, 0].sum()
    giniSum = all[:, 0].cumsum().sum() / totalLosses

    giniSum -= (len(actual) + 1) / 2.
    return giniSum / len(actual)


def gini_normalized(actual, pred):
    return gini(actual, pred) / gini(actual, actual)

train_df = pd.read_csv('./InsNova_data_2023_train.csv')
train_df['logclaimcost'] = [0 if x == 0 else math.log(x) for x in train_df['claimcst0'].values]

numeric_columns = ['veh_value', 'max_power', 'credit_score']
for column in numeric_columns:
    train_df[column] = np.log1p(train_df[column])

categorical_columns = [
    'gender', 'marital_status','e_bill', 'time_of_week_driven', 'trm_len', 'high_education_ind',
    'veh_age', 'agecat', 'veh_body', 'area', 'engine_type', 'veh_color', 'time_driven']

for column in categorical_columns:
    train_df[column] = LabelEncoder().fit_transform(train_df[column].astype(str))

y = train_df['logclaimcost'].values
features_to_drop = ['claimcst0', 'clm', 'numclaims', 'id', 'logclaimcost']
X = train_df.drop(features_to_drop, axis=1)

# tuned parameters
best_params = {
    'feature_fraction': 0.6,
    'learning_rate': 0.01,
    'n_estimators': 200,
    'num_leaves': 31
}

best_params_xgb = {
    'max_depth': 3,
    'learning_rate': 0.032,
    'n_estimators': 100,
    'subsample': 1.0,
    'colsample_bytree': 0.93,
    'gamma': 0.59
}

# K-Fold cross-validation
n_splits = 10
kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

# Initialize OOF predictions arrays
oof_preds = {
    'rf': np.zeros(len(y)),
    'mlp': np.zeros(len(y)),
    'xgb': np.zeros(len(y)),
    'lr': np.zeros(len(y)),
    'svr': np.zeros(len(y)),
    'lgb': np.zeros(len(y)),
    'twe': np.zeros(len(y))
}

gini_scores = []

for train_index, val_index in kf.split(X):
    X_train, X_val = X.iloc[train_index], X.iloc[val_index]
    y_train, y_val = y[train_index], y[val_index]

    # Initialize models
    model1 = RandomForestRegressor(max_depth=5)
    model2 = MLPRegressor(random_state=1, max_iter=500, hidden_layer_sizes=(100, 64, 64, 100))
    model3 = xgb.XGBRegressor(**best_params_xgb)
    model4 = LinearRegression()
    model5 = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))
    lgb_model = lgb.LGBMRegressor(**best_params)
    tweedie_model = sm.GLM(y_train, X_train, family=sm.families.Tweedie(link=sm.families.links.log(), var_power=1.5))

    # Fit models
    model1.fit(X_train, y_train)
    model2.fit(X_train, y_train)
    model3.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=50, verbose=False)
    model4.fit(X_train, y_train)
    model5.fit(X_train, y_train)
    lgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)],
                  callbacks=[early_stopping(stopping_rounds=50)])
    tweedie_results = tweedie_model.fit(maxiter=1000)

    # Update Oout Of Fold predictions
    oof_preds['rf'][val_index] = model1.predict(X_val)
    oof_preds['mlp'][val_index] = model2.predict(X_val)
    oof_preds['xgb'][val_index] = model3.predict(X_val)
    oof_preds['lr'][val_index] = model4.predict(X_val)
    oof_preds['svr'][val_index] = model5.predict(X_val)
    oof_preds['lgb'][val_index] = lgb_model.predict(X_val)
    oof_preds['twe'][val_index] = tweedie_results.predict(X_val)

from sklearn.metrics import mean_squared_error

# Define the weights for each model
w_rf = 0.1
w_mlp = 0.05
w_xgb = 0.2
w_lr = 0.1
w_svr = 0.05
w_lgb = 0.3
w_twe = 0.2

# Ensure that the sum of weights equals 1
assert w_rf + w_mlp + w_xgb + w_lr + w_svr + w_lgb + w_twe == 1

combined_oof = (w_rf * oof_preds['rf'] +
                w_mlp * oof_preds['mlp'] +
                w_xgb * oof_preds['xgb'] +
                w_lr * oof_preds['lr'] +
                w_svr * oof_preds['svr'] +
                w_lgb * oof_preds['lgb'] +
                w_twe * oof_preds['twe'])

mse_oof = mean_squared_error(y, combined_oof)
print("Combined OOF MSE: {:.4f}".format(mse_oof))

gini_score_oof = gini_normalized(np.exp(y), np.exp(combined_oof))
print("Combined OOF Gini Score: {:.4f}".format(gini_score_oof))

validation_df = pd.read_csv('/content/InsNova_data_2023_vh.csv')
validation_df['logclaimcost'] = [0 if x == 0 else math.log(x) for x in validation_df['claimcst0'].values]

scaler = RobustScaler()
# Normalize skewed numeric predictors using RobustScaler
numeric_columns = ['veh_value', 'max_power', 'credit_score']
for column in numeric_columns:
    train_df[column] = scaler.fit_transform(train_df[column].values.reshape(-1, 1))

# Encoding for categorical columns
ordinal_columns = ['veh_age', 'agecat']
binary_columns = ['gender', 'marital_status', 'e_bill', 'time_of_week_driven', 'trm_len', 'high_education_ind']
nominal_columns = ['area', 'time_driven', 'engine_type', 'veh_color', 'veh_body']

for column in ordinal_columns + binary_columns:
    validation_df[column] = LabelEncoder().fit_transform(validation_df[column])

for column in nominal_columns:
    validation_df = pd.concat([validation_df, pd.get_dummies(validation_df[column], prefix=column)], axis=1)

features_to_drop = ['claimcst0', 'clm', 'numclaims', 'id', 'logclaimcost', 'marital_status', 'e_bill', 'veh_color'] + nominal_columns
X_validation = validation_df.drop(features_to_drop, axis=1)

y_preds_rf = model1.predict(X_validation)
y_preds_mlp = model2.predict(X_validation)
y_preds_xgb = model3.predict(X_validation)
y_preds_lr = model4.predict(X_validation)
y_preds_svr = model5.predict(X_validation)
y_preds_lgb = lgb_model.predict(X_validation)
tweedie_results = tweedie_model.predict(X_validation)

w_rf = 0.1
w_mlp = 0.05
w_xgb = 0.2
w_lr = 0.1
w_svr = 0.05
w_lgb = 0.3
w_twe = 0.2

y_combined = (w_rf * y_preds_rf +
              w_mlp * y_preds_mlp +
              w_xgb * y_preds_xgb +
              w_lr * y_preds_lr +
              w_svr * y_preds_svr +
              w_lgb * y_preds_lgb +
              w_twe * tweedie_results)

y_scaled = [np.exp(i) for i in y_combined]

submission_df = pd.DataFrame()
submission_df['id'] = validation_df['id']
submission_df['Predict'] = y_scaled

submission_df.to_csv("submission_stack.csv", index=False)